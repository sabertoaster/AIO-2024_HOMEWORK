{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN_YdPH6q-gE",
        "outputId": "b4b353de-5cfa-47f3-de55-698078ae056f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SqSn_8rxkk-Qvu4JLMcN_3ZFGDNa6P_V\n",
            "To: /content/NonLinear_data.npy\n",
            "100% 5.57k/5.57k [00:00<00:00, 14.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# 1. Download dataset\n",
        "!gdown --id 1SqSn_8rxkk-Qvu4JLMcN_3ZFGDNa6P_V\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "MfM_FF4frCN5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Set random seed and computation device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "random_state = 59\n",
        "np.random.seed(random_state)\n",
        "torch.manual_seed(random_state)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(random_state)"
      ],
      "metadata": {
        "id": "iT0wXwpsrFZ0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Read dataset\n",
        "data_path = '/content/NonLinear_data.npy'\n",
        "data = np.load(data_path, allow_pickle=True).item()\n",
        "X, y = data['X'], data['labels']\n",
        "\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PheHCcRXrHN-",
        "outputId": "475adf3b-c062-4066-cf8e-0721f588aaf9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 2) (300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Split train/val/test sets\n",
        "val_size = 0.2\n",
        "test_size = 0.125\n",
        "is_shuffle = True\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=val_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train, y_train,\n",
        "    test_size=test_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")"
      ],
      "metadata": {
        "id": "bu1vwWEOrJ6q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Feature normalization\n",
        "normalizer = StandardScaler()\n",
        "X_train = normalizer.fit_transform(X_train)\n",
        "X_val = normalizer.transform(X_val)\n",
        "X_test = normalizer.transform(X_test)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)"
      ],
      "metadata": {
        "id": "Mk1szif4rMV4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Build DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "batch_size = 32\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "val_dataset = CustomDataset(X_val, y_val)\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                       batch_size=batch_size,\n",
        "                       shuffle=False)\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False)"
      ],
      "metadata": {
        "id": "d_eY9Nv9rXl2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Build MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
        "        self.output = nn.Linear(hidden_dims, output_dims)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        out = self.output(x)\n",
        "        return out.squeeze(1)\n",
        "\n",
        "input_dims = X_train.shape[1]\n",
        "output_dims = torch.unique(y_train).shape[0]\n",
        "hidden_dims = 128\n",
        "\n",
        "model = MLP(input_dims=input_dims,\n",
        "            hidden_dims=hidden_dims,\n",
        "            output_dims=output_dims).to(device)"
      ],
      "metadata": {
        "id": "TWIYG4PTrT2d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Define loss function and optimizer\n",
        "lr = 1e-1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "--03YevorSoq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Build accuracy computation function\n",
        "def compute_accuracy(y_hat, y_true):\n",
        "    _, y_hat = torch.max(y_hat, dim=1)\n",
        "    correct = (y_hat == y_true).sum().item()\n",
        "    accuracy = correct / len(y_true)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "q-tXBy86rQ6W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Train model\n",
        "epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_target = []\n",
        "    train_predict = []\n",
        "    model.train()\n",
        "    for X_samples, y_samples in train_loader:\n",
        "        X_samples = X_samples.to(device)\n",
        "        y_samples = y_samples.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_samples)\n",
        "        loss = criterion(outputs, y_samples)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        train_predict.append(outputs.detach().cpu())\n",
        "        train_target.append(y_samples.cpu())\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    train_predict = torch.cat(train_predict)\n",
        "    train_target = torch.cat(train_target)\n",
        "    train_acc = compute_accuracy(train_predict, train_target)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_target = []\n",
        "    val_predict = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_samples, y_samples in val_loader:\n",
        "            X_samples = X_samples.to(device)\n",
        "            y_samples = y_samples.to(device)\n",
        "            outputs = model(X_samples)\n",
        "            val_loss += criterion(outputs, y_samples).item()\n",
        "\n",
        "            val_predict.append(outputs.cpu())\n",
        "            val_target.append(y_samples.cpu())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        val_predict = torch.cat(val_predict)\n",
        "        val_target = torch.cat(val_target)\n",
        "        val_acc = compute_accuracy(val_predict, val_target)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "    print(f'\\nEPOCH {epoch + 1}:\\tTraining loss: {train_loss:.3f}\\tValidation loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccEY5zCCrPRO",
        "outputId": "d0d6247c-45ae-43de-867a-28aaf2853245"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 1:\tTraining loss: 0.939\tValidation loss: 0.864\n",
            "\n",
            "EPOCH 2:\tTraining loss: 0.750\tValidation loss: 0.815\n",
            "\n",
            "EPOCH 3:\tTraining loss: 0.715\tValidation loss: 0.799\n",
            "\n",
            "EPOCH 4:\tTraining loss: 0.689\tValidation loss: 0.791\n",
            "\n",
            "EPOCH 5:\tTraining loss: 0.670\tValidation loss: 0.780\n",
            "\n",
            "EPOCH 6:\tTraining loss: 0.654\tValidation loss: 0.747\n",
            "\n",
            "EPOCH 7:\tTraining loss: 0.646\tValidation loss: 0.724\n",
            "\n",
            "EPOCH 8:\tTraining loss: 0.616\tValidation loss: 0.704\n",
            "\n",
            "EPOCH 9:\tTraining loss: 0.616\tValidation loss: 0.694\n",
            "\n",
            "EPOCH 10:\tTraining loss: 0.610\tValidation loss: 0.666\n",
            "\n",
            "EPOCH 11:\tTraining loss: 0.577\tValidation loss: 0.666\n",
            "\n",
            "EPOCH 12:\tTraining loss: 0.596\tValidation loss: 0.642\n",
            "\n",
            "EPOCH 13:\tTraining loss: 0.556\tValidation loss: 0.625\n",
            "\n",
            "EPOCH 14:\tTraining loss: 0.537\tValidation loss: 0.608\n",
            "\n",
            "EPOCH 15:\tTraining loss: 0.521\tValidation loss: 0.604\n",
            "\n",
            "EPOCH 16:\tTraining loss: 0.530\tValidation loss: 0.587\n",
            "\n",
            "EPOCH 17:\tTraining loss: 0.511\tValidation loss: 0.566\n",
            "\n",
            "EPOCH 18:\tTraining loss: 0.498\tValidation loss: 0.569\n",
            "\n",
            "EPOCH 19:\tTraining loss: 0.493\tValidation loss: 0.540\n",
            "\n",
            "EPOCH 20:\tTraining loss: 0.480\tValidation loss: 0.542\n",
            "\n",
            "EPOCH 21:\tTraining loss: 0.482\tValidation loss: 0.520\n",
            "\n",
            "EPOCH 22:\tTraining loss: 0.462\tValidation loss: 0.501\n",
            "\n",
            "EPOCH 23:\tTraining loss: 0.443\tValidation loss: 0.488\n",
            "\n",
            "EPOCH 24:\tTraining loss: 0.441\tValidation loss: 0.475\n",
            "\n",
            "EPOCH 25:\tTraining loss: 0.420\tValidation loss: 0.473\n",
            "\n",
            "EPOCH 26:\tTraining loss: 0.420\tValidation loss: 0.459\n",
            "\n",
            "EPOCH 27:\tTraining loss: 0.424\tValidation loss: 0.449\n",
            "\n",
            "EPOCH 28:\tTraining loss: 0.399\tValidation loss: 0.437\n",
            "\n",
            "EPOCH 29:\tTraining loss: 0.387\tValidation loss: 0.429\n",
            "\n",
            "EPOCH 30:\tTraining loss: 0.386\tValidation loss: 0.419\n",
            "\n",
            "EPOCH 31:\tTraining loss: 0.380\tValidation loss: 0.411\n",
            "\n",
            "EPOCH 32:\tTraining loss: 0.369\tValidation loss: 0.403\n",
            "\n",
            "EPOCH 33:\tTraining loss: 0.359\tValidation loss: 0.397\n",
            "\n",
            "EPOCH 34:\tTraining loss: 0.351\tValidation loss: 0.389\n",
            "\n",
            "EPOCH 35:\tTraining loss: 0.354\tValidation loss: 0.382\n",
            "\n",
            "EPOCH 36:\tTraining loss: 0.344\tValidation loss: 0.374\n",
            "\n",
            "EPOCH 37:\tTraining loss: 0.335\tValidation loss: 0.374\n",
            "\n",
            "EPOCH 38:\tTraining loss: 0.336\tValidation loss: 0.365\n",
            "\n",
            "EPOCH 39:\tTraining loss: 0.336\tValidation loss: 0.354\n",
            "\n",
            "EPOCH 40:\tTraining loss: 0.317\tValidation loss: 0.346\n",
            "\n",
            "EPOCH 41:\tTraining loss: 0.314\tValidation loss: 0.340\n",
            "\n",
            "EPOCH 42:\tTraining loss: 0.307\tValidation loss: 0.341\n",
            "\n",
            "EPOCH 43:\tTraining loss: 0.309\tValidation loss: 0.336\n",
            "\n",
            "EPOCH 44:\tTraining loss: 0.290\tValidation loss: 0.327\n",
            "\n",
            "EPOCH 45:\tTraining loss: 0.288\tValidation loss: 0.327\n",
            "\n",
            "EPOCH 46:\tTraining loss: 0.286\tValidation loss: 0.318\n",
            "\n",
            "EPOCH 47:\tTraining loss: 0.290\tValidation loss: 0.310\n",
            "\n",
            "EPOCH 48:\tTraining loss: 0.285\tValidation loss: 0.305\n",
            "\n",
            "EPOCH 49:\tTraining loss: 0.268\tValidation loss: 0.302\n",
            "\n",
            "EPOCH 50:\tTraining loss: 0.274\tValidation loss: 0.299\n",
            "\n",
            "EPOCH 51:\tTraining loss: 0.270\tValidation loss: 0.293\n",
            "\n",
            "EPOCH 52:\tTraining loss: 0.267\tValidation loss: 0.288\n",
            "\n",
            "EPOCH 53:\tTraining loss: 0.255\tValidation loss: 0.290\n",
            "\n",
            "EPOCH 54:\tTraining loss: 0.262\tValidation loss: 0.284\n",
            "\n",
            "EPOCH 55:\tTraining loss: 0.245\tValidation loss: 0.284\n",
            "\n",
            "EPOCH 56:\tTraining loss: 0.247\tValidation loss: 0.277\n",
            "\n",
            "EPOCH 57:\tTraining loss: 0.248\tValidation loss: 0.271\n",
            "\n",
            "EPOCH 58:\tTraining loss: 0.240\tValidation loss: 0.269\n",
            "\n",
            "EPOCH 59:\tTraining loss: 0.240\tValidation loss: 0.263\n",
            "\n",
            "EPOCH 60:\tTraining loss: 0.241\tValidation loss: 0.261\n",
            "\n",
            "EPOCH 61:\tTraining loss: 0.237\tValidation loss: 0.270\n",
            "\n",
            "EPOCH 62:\tTraining loss: 0.230\tValidation loss: 0.262\n",
            "\n",
            "EPOCH 63:\tTraining loss: 0.233\tValidation loss: 0.260\n",
            "\n",
            "EPOCH 64:\tTraining loss: 0.226\tValidation loss: 0.253\n",
            "\n",
            "EPOCH 65:\tTraining loss: 0.231\tValidation loss: 0.250\n",
            "\n",
            "EPOCH 66:\tTraining loss: 0.222\tValidation loss: 0.246\n",
            "\n",
            "EPOCH 67:\tTraining loss: 0.220\tValidation loss: 0.246\n",
            "\n",
            "EPOCH 68:\tTraining loss: 0.216\tValidation loss: 0.243\n",
            "\n",
            "EPOCH 69:\tTraining loss: 0.215\tValidation loss: 0.239\n",
            "\n",
            "EPOCH 70:\tTraining loss: 0.213\tValidation loss: 0.243\n",
            "\n",
            "EPOCH 71:\tTraining loss: 0.210\tValidation loss: 0.239\n",
            "\n",
            "EPOCH 72:\tTraining loss: 0.203\tValidation loss: 0.239\n",
            "\n",
            "EPOCH 73:\tTraining loss: 0.203\tValidation loss: 0.236\n",
            "\n",
            "EPOCH 74:\tTraining loss: 0.210\tValidation loss: 0.237\n",
            "\n",
            "EPOCH 75:\tTraining loss: 0.204\tValidation loss: 0.232\n",
            "\n",
            "EPOCH 76:\tTraining loss: 0.200\tValidation loss: 0.227\n",
            "\n",
            "EPOCH 77:\tTraining loss: 0.200\tValidation loss: 0.226\n",
            "\n",
            "EPOCH 78:\tTraining loss: 0.195\tValidation loss: 0.227\n",
            "\n",
            "EPOCH 79:\tTraining loss: 0.186\tValidation loss: 0.222\n",
            "\n",
            "EPOCH 80:\tTraining loss: 0.186\tValidation loss: 0.220\n",
            "\n",
            "EPOCH 81:\tTraining loss: 0.194\tValidation loss: 0.214\n",
            "\n",
            "EPOCH 82:\tTraining loss: 0.189\tValidation loss: 0.214\n",
            "\n",
            "EPOCH 83:\tTraining loss: 0.183\tValidation loss: 0.212\n",
            "\n",
            "EPOCH 84:\tTraining loss: 0.188\tValidation loss: 0.212\n",
            "\n",
            "EPOCH 85:\tTraining loss: 0.181\tValidation loss: 0.210\n",
            "\n",
            "EPOCH 86:\tTraining loss: 0.180\tValidation loss: 0.211\n",
            "\n",
            "EPOCH 87:\tTraining loss: 0.178\tValidation loss: 0.216\n",
            "\n",
            "EPOCH 88:\tTraining loss: 0.181\tValidation loss: 0.206\n",
            "\n",
            "EPOCH 89:\tTraining loss: 0.178\tValidation loss: 0.202\n",
            "\n",
            "EPOCH 90:\tTraining loss: 0.171\tValidation loss: 0.201\n",
            "\n",
            "EPOCH 91:\tTraining loss: 0.180\tValidation loss: 0.213\n",
            "\n",
            "EPOCH 92:\tTraining loss: 0.172\tValidation loss: 0.201\n",
            "\n",
            "EPOCH 93:\tTraining loss: 0.166\tValidation loss: 0.195\n",
            "\n",
            "EPOCH 94:\tTraining loss: 0.173\tValidation loss: 0.196\n",
            "\n",
            "EPOCH 95:\tTraining loss: 0.169\tValidation loss: 0.194\n",
            "\n",
            "EPOCH 96:\tTraining loss: 0.165\tValidation loss: 0.193\n",
            "\n",
            "EPOCH 97:\tTraining loss: 0.163\tValidation loss: 0.192\n",
            "\n",
            "EPOCH 98:\tTraining loss: 0.161\tValidation loss: 0.193\n",
            "\n",
            "EPOCH 99:\tTraining loss: 0.162\tValidation loss: 0.188\n",
            "\n",
            "EPOCH 100:\tTraining loss: 0.162\tValidation loss: 0.190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Evaluate model\n",
        "test_target = []\n",
        "test_predict = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X_samples, y_samples in test_loader:\n",
        "        X_samples = X_samples.to(device)\n",
        "        y_samples = y_samples.to(device)\n",
        "        outputs = model(X_samples)\n",
        "\n",
        "        test_predict.append(outputs.cpu())\n",
        "        test_target.append(y_samples.cpu())\n",
        "\n",
        "    test_predict = torch.cat(test_predict)\n",
        "    test_target = torch.cat(test_target)\n",
        "    test_acc = compute_accuracy(test_predict, test_target)\n",
        "\n",
        "    print('Evaluation on test set:')\n",
        "    print(f'Accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpnvSx41rNmK",
        "outputId": "1f6c5476-0867-4b80-ec49-de7f4815d03f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test set:\n",
            "Accuracy: 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YbIaBF6MsIzU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}