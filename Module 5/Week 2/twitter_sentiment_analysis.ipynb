{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ],
   "id": "4886d81d9069ddc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read data\n",
    "dataset_path = 'Twitter_Data.csv'\n",
    "df = pd.read_csv(\n",
    "    dataset_path\n",
    ")"
   ],
   "id": "2731ea6006a625fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove missing data\n",
    "df = df.dropna()"
   ],
   "id": "d02731f2316e324c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Text preprocessing function\n",
    "def text_normalize(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Retweet old acronym \"RT\" removal\n",
    "    text = re.sub(r'^rt[\\s]+', '', text)\n",
    "    \n",
    "    # Hyperlinks removal\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    \n",
    "    # Punctuation removal\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ],
   "id": "bf2f83d2abc77c8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create tf-idf features\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X = vectorizer.fit_transform(df['clean_text']).toarray()"
   ],
   "id": "f8f5fe37d6b0683"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add bias term\n",
    "intercept = np.ones((\n",
    "    X.shape[0], 1)\n",
    ")\n",
    "X_b = np.concatenate(\n",
    "    (intercept, X),\n",
    "    axis=1\n",
    ")"
   ],
   "id": "a0332592ef759f5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# One-hot encoding label\n",
    "n_classes = df['category'].nunique()\n",
    "n_samples = df['category'].size\n",
    "\n",
    "y = df['category'].to_numpy() + 1\n",
    "y = y.astype(np.uint8)\n",
    "y_encoded = np.array(\n",
    "    [np.zeros(n_classes) for _ in range(n_samples)]\n",
    ")\n",
    "y_encoded[np.arange(n_samples), y] = 1"
   ],
   "id": "b3a4de9e06171b68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split train/val/test sets\n",
    "val_size = 0.2\n",
    "test_size = 0.125\n",
    "random_state = 2\n",
    "is_shuffle = True\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_b, y_encoded,\n",
    "    test_size=val_size,\n",
    "    random_state=random_state,\n",
    "    shuffle=is_shuffle\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    shuffle=is_shuffle\n",
    ")"
   ],
   "id": "118579a9969b273e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define core functions\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / exp_z.sum(axis=1)[:, None]\n",
    "\n",
    "def predict(X, theta):\n",
    "    z = np.dot(X, theta)\n",
    "    y_hat = softmax(z)\n",
    "    return y_hat\n",
    "\n",
    "def compute_loss(y_hat, y):\n",
    "    n = y.size\n",
    "    return (-1 / n) * np.sum(y * np.log(y_hat))\n",
    "\n",
    "def compute_gradient(X, y, y_hat):\n",
    "    n = y.size\n",
    "    return np.dot(X.T, (y_hat - y)) / n\n",
    "\n",
    "def update_theta(theta, gradient, lr):\n",
    "    return theta - lr * gradient\n",
    "\n",
    "def compute_accuracy(X, y, theta):\n",
    "    y_hat = predict(X, theta)\n",
    "    acc = (np.argmax(y_hat, axis=1) == np.argmax(y, axis=1)).mean()\n",
    "    return acc"
   ],
   "id": "495a7ce86bbd76d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize parameters\n",
    "lr = 0.1\n",
    "epochs = 200\n",
    "batch_size = X_train.shape[0]\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "np.random.seed(random_state)\n",
    "theta = np.random.uniform(\n",
    "    size=(n_features, n_classes)\n",
    ")"
   ],
   "id": "b20a7b2e5884168"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training loop\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_batch_losses = []\n",
    "    train_batch_accs = []\n",
    "    val_batch_losses = []\n",
    "    val_batch_accs = []\n",
    "\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_i = X_train[i:i+batch_size]\n",
    "        y_i = y_train[i:i+batch_size]\n",
    "\n",
    "        y_hat = predict(X_i, theta)\n",
    "        train_loss = compute_loss(y_hat, y_i)\n",
    "        gradient = compute_gradient(X_i, y_i, y_hat)\n",
    "        theta = update_theta(theta, gradient, lr)\n",
    "        \n",
    "        train_batch_losses.append(train_loss)\n",
    "        train_acc = compute_accuracy(X_train, y_train, theta)\n",
    "        train_batch_accs.append(train_acc)\n",
    "\n",
    "        y_val_hat = predict(X_val, theta)\n",
    "        val_loss = compute_loss(y_val_hat, y_val)\n",
    "        val_batch_losses.append(val_loss)\n",
    "        \n",
    "        val_acc = compute_accuracy(X_val, y_val, theta)\n",
    "        val_batch_accs.append(val_acc)\n",
    "\n",
    "    train_batch_loss = sum(train_batch_losses) / len(train_batch_losses)\n",
    "    val_batch_loss = sum(val_batch_losses) / len(val_batch_losses)\n",
    "    train_batch_acc = sum(train_batch_accs) / len(train_batch_accs)\n",
    "    val_batch_acc = sum(val_batch_accs) / len(val_batch_accs)\n",
    "\n",
    "    train_losses.append(train_batch_loss)\n",
    "    val_losses.append(val_batch_loss)\n",
    "    train_accs.append(train_batch_acc)\n",
    "    val_accs.append(val_batch_acc)\n",
    "    \n",
    "    print(f'\\nEPOCH {epoch + 1}:\\tTraining loss: {train_batch_loss:.3f}\\tValidation loss: {val_batch_loss:.3f}')"
   ],
   "id": "806d4eff4febb40d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "ax[0, 0].plot(train_losses)\n",
    "ax[0, 0].set(xlabel='Epoch', ylabel='Loss')\n",
    "ax[0, 0].set_title('Training Loss')\n",
    "\n",
    "ax[0, 1].plot(val_losses, 'orange')\n",
    "ax[0, 1].set(xlabel='Epoch', ylabel='Loss')\n",
    "ax[0, 1].set_title('Validation Loss')\n",
    "\n",
    "ax[1, 0].plot(train_accs)\n",
    "ax[1, 0].set(xlabel='Epoch', ylabel='Accuracy')\n",
    "ax[1, 0].set_title('Training Accuracy')\n",
    "\n",
    "ax[1, 1].plot(val_accs, 'orange')\n",
    "ax[1, 1].set(xlabel='Epoch', ylabel='Accuracy')\n",
    "ax[1, 1].set_title('Validation Accuracy')\n",
    "\n",
    "plt.show()"
   ],
   "id": "4ab44028d1fab610"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate model\n",
    "val_set_acc = compute_accuracy(X_val, y_val, theta)\n",
    "test_set_acc = compute_accuracy(X_test, y_test, theta)\n",
    "print('Evaluation on validation and test set:')\n",
    "print(f'Accuracy: {val_set_acc}')\n",
    "print(f'Accuracy: {test_set_acc}')"
   ],
   "id": "c6276804f32588c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
