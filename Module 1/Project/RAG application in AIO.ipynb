{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jn73A-cGhzE"
      },
      "source": [
        "# 1. Preparing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR9tw_guGhzG",
        "outputId": "3a21705e-f014-4f6d-c7aa-eb26b167cab6"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.41.2\n",
        "!pip install -q bitsandbytes==0.43.1\n",
        "!pip install -q accelerate==0.31.0\n",
        "!pip install -q langchain==0.2.5\n",
        "!pip install -q langchainhub==0.1.20\n",
        "!pip install -q langchain-chroma==0.1.1\n",
        "!pip install -q langchain-community==0.2.5\n",
        "!pip install -q langchain_huggingface==0.0.3\n",
        "!pip install -q python-dotenv==1.0.1\n",
        "!pip install -q pypdf==4.2.0\n",
        "!pip install -q numpy==1.24.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Vector Databases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### a) Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbAWPP7hGhzH",
        "outputId": "2a2fdc55-0c68-42b2-ee37-200f35b357f4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoTokenizer , AutoModelForCausalLM , pipeline\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_community.document_loaders import PyPDFLoader , TextLoader\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### b) Read pdf file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NB45w2_FGhzI"
      },
      "outputs": [],
      "source": [
        "Loader = PyPDFLoader\n",
        "FILE_PATH = \"./AIO-2024-All-Materials.pdf\"\n",
        "loader = Loader(FILE_PATH)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### c) Text splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKTt2cLOGkZs"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                               chunk_overlap=100)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "print(\" Number of sub - documents : \", len(docs))\n",
        "print(docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### d) Instance vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egawCpAtHHUR"
      },
      "outputs": [],
      "source": [
        "embedding = HuggingFaceEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### e) Vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_db = Chroma.from_documents(documents=docs,\n",
        "                                  embedding=embedding)\n",
        "retriever = vector_db.as_retriever()\n",
        "result = retriever.invoke(\" What is YOLO ?\")\n",
        "print(\" Number of relevant documents : \", len(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkyWsnOKKgPU"
      },
      "source": [
        "# 3. LLMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lu4dAdpglEt"
      },
      "outputs": [],
      "source": [
        "nf4_config = BitsAndBytesConfig (\n",
        "    load_in_4bit =True ,\n",
        "    bnb_4bit_quant_type =\"nf4\",\n",
        "    bnb_4bit_use_double_quant =True ,\n",
        "    bnb_4bit_compute_dtype = torch.bfloat16\n",
        ")\n",
        "MODEL_NAME = \"lmsys/vicuna-7b-v1.5\"\n",
        "model = AutoModelForCausalLM.from_pretrained (\n",
        "    MODEL_NAME ,\n",
        "    quantization_config = nf4_config ,\n",
        "    low_cpu_mem_usage = True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained (model_name)\n",
        "low_cpu_mem_usage = True\n",
        "tokenizer = AutoTokenizer.from_pretrained (model_name)\n",
        "model_pipeline = pipeline (\n",
        "    \"text-generation \",\n",
        "    model = model ,\n",
        "    tokenizer = tokenizer ,\n",
        "    max_new_tokens = 512 ,\n",
        "    pad_token_id = tokenizer.eos_token_id ,\n",
        "    device_map = \"auto \"\n",
        ")\n",
        "llm = HuggingFacePipeline (\n",
        "    pipeline = model_pipeline ,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8-p1MCNKv9y"
      },
      "source": [
        "# 4. Run the program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjduf70x1pYq",
        "outputId": "9abf47a7-f2ce-4315-961c-6b2f4fa5839c"
      },
      "outputs": [],
      "source": [
        "prompt = hub.pull (\"rlm/rag-prompt\")\n",
        "def format_docs ( docs ):\n",
        "    return \"\\n\\n\". join (doc. page_content for doc in docs )\n",
        "rag_chain = (\n",
        "    {\" context \": retriever | format_docs , \" question \": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser ()\n",
        ")\n",
        "USER_QUESTION = \" YOLOv10 là gì?\"\n",
        "output = rag_chain.invoke( USER_QUESTION )\n",
        "answer = output.split ('Answer :') [1].strip()\n",
        "print ( answer )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCH1-mUBoCnx"
      },
      "outputs": [],
      "source": [
        "!mkdir safety_helmet_dataset\n",
        "%cd /content/safety_helmet_dataset\n",
        "!gdown '1twdtZEfcw4ghSZIiPDypJurZnNXzMO7R'\n",
        "!unzip '/content/Safety_Helmet_Dataset.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmlFJObxJbOy"
      },
      "source": [
        "# Build the chat UI using streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inLWDU-er_YI",
        "outputId": "f1da862c-d881-4a1d-f626-8b42775bd991"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers ==4.41.2\n",
        "!pip install -q bitsandbytes ==0.43.1\n",
        "!pip install -q accelerate ==0.31.0\n",
        "!pip install -q langchain ==0.2.5\n",
        "!pip install -q langchainhub ==0.1.20\n",
        "!pip install -q langchain - chroma ==0.1.1\n",
        "!pip install -q langchain - community ==0.2.5\n",
        "!pip install -q langchain - chroma ==0.1.1\n",
        "!pip install -q langchain - community ==0.2.5\n",
        "!pip install -q langchain - openai ==0.1.9\n",
        "!pip install -q langchain_huggingface ==0.0.3\n",
        "!pip install -q chainlit ==1.1.304\n",
        "!pip install -q python - dotenv ==1.0.1\n",
        "!pip install -q pypdf ==4.2.0\n",
        "!pip install -q numpy ==1.24.4\n",
        "!npm install -g localtunnel\n",
        "\n",
        "import chainlit as cl\n",
        "import torch\n",
        "from chainlit . types import AskFileResponse\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoTokenizer , AutoModelForCausalLM , pipeline\n",
        "from langchain_huggingface . llms import HuggingFacePipeline\n",
        "from langchain . memory import ConversationBufferMemory\n",
        "from langchain_community . chat_message_histories import ChatMessageHistory\n",
        "from langchain . chains import ConversationalRetrievalChain\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community . document_loaders import PyPDFLoader , TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core . runnables import RunnablePassthrough\n",
        "from langchain_core . output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter (chunk_size =1000, chunk_overlap =100)\n",
        "embedding = HuggingFaceEmbeddings ()\n",
        "\n",
        "def process_file ( file : AskFileResponse ):\n",
        "    if file . type == \"text/plain\":\n",
        "        Loader = TextLoader\n",
        "    elif file . type == \"application/pdf\":\n",
        "        Loader = PyPDFLoader\n",
        "    loader = Loader ( file . path )\n",
        "    documents = loader.load()\n",
        "    docs = text_splitter.split_documents ( documents )\n",
        "    for i, doc in enumerate ( docs ):\n",
        "        doc.metadata [\"source\"] = f\" source_{i}\"\n",
        "    return docs\n",
        "\n",
        "def get_vector_db ( file : AskFileResponse ):\n",
        "    docs = process_file ( file )\n",
        "    cl. user_session .set (\"docs\", docs )\n",
        "    vector_db = Chroma . from_documents ( documents =docs ,\n",
        "    embedding = embedding )\n",
        "    return vector_db\n",
        "\n",
        "def get_huggingface_llm ( model_name : str = \"lmsys /vicuna-7b-v1.5\",\n",
        "max_new_token : int = 512) :\n",
        "    nf4_config = BitsAndBytesConfig (\n",
        "    load_in_4bit =True ,\n",
        "    bnb_4bit_quant_type =\"nf4\",\n",
        "    bnb_4bit_use_double_quant =True ,\n",
        "    bnb_4bit_compute_dtype = torch . bfloat16\n",
        "    )\n",
        "    model = AutoModelForCausalLM . from_pretrained (\n",
        "    model_name ,\n",
        "    quantization_config = nf4_config ,\n",
        "    low_cpu_mem_usage = True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer . from_pretrained ( model_name )\n",
        "    model_pipeline = pipeline (\n",
        "    \"text-generation\",\n",
        "    model =model ,\n",
        "    tokenizer = tokenizer ,\n",
        "    max_new_tokens = max_new_token ,\n",
        "    pad_token_id = tokenizer . eos_token_id ,\n",
        "    device_map =\"auto\"\n",
        "    )\n",
        "    llm = HuggingFacePipeline (\n",
        "    pipeline = model_pipeline ,\n",
        "    )\n",
        "    return llm\n",
        "LLM = get_huggingface_llm ()\n",
        "welcome_message = \"\"\" Welcome to the PDF QA! To get started :\n",
        "1. Upload a PDF or text file\n",
        "2. Ask a question about the file\n",
        "\"\"\"\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def on_chat_start ():\n",
        "    files = None\n",
        "    while files is None :\n",
        "        files = await cl. AskFileMessage (\n",
        "        content = welcome_message ,\n",
        "        accept = [\"text/plain\", \"application/pdf\"],\n",
        "        max_size_mb =20 ,\n",
        "        timeout =180 ,\n",
        "        ). send ()\n",
        "    file = files [0]\n",
        "    msg = cl. Message ( content =f\" Processing '{ file . name }'...\",\n",
        "    disable_feedback = True )\n",
        "    await msg . send ()\n",
        "    vector_db = await cl. make_async ( get_vector_db )( file )\n",
        "    message_history = ChatMessageHistory ()\n",
        "    memory = ConversationBufferMemory (\n",
        "    memory_key =\"chat_history\",\n",
        "    output_key =\"answer\",\n",
        "    chat_memory = message_history ,\n",
        "    return_messages =True ,\n",
        "    )\n",
        "    retriever = vector_db . as_retriever ( search_type =\"mmr\",\n",
        "    search_kwargs ={ 'k': 3})\n",
        "    chain = ConversationalRetrievalChain . from_llm (\n",
        "    llm =LLM ,\n",
        "    chain_type =\"stuff\",\n",
        "    retriever = retriever ,\n",
        "    memory =memory ,\n",
        "    return_source_documents = True\n",
        "    )\n",
        "    msg . content = f\" '{ file . name }' processed . You can now ask questions !\"\n",
        "    await msg . update ()\n",
        "    cl. user_session .set (\"chain\", chain )\n",
        "    \n",
        "@cl.on_message\n",
        "async def on_message ( message : cl. Message ):\n",
        "    chain = cl. user_session .get (\" chain \")\n",
        "    cb = cl. AsyncLangchainCallbackHandler ()\n",
        "    res = await chain . ainvoke ( message . content , callbacks =[ cb ])\n",
        "    answer = res[\" answer \"]\n",
        "    source_documents = res[\"source_documents\"]\n",
        "    text_elements = []\n",
        "    if source_documents :\n",
        "        for source_idx , source_doc in enumerate ( source_documents ):\n",
        "            source_name = f\" source_ { source_idx }\"\n",
        "            text_elements . append (\n",
        "            cl. Text ( content = source_doc . page_content ,\n",
        "            name = source_name )\n",
        "            )\n",
        "        source_names = [ text_el . name for text_el in text_elements ]\n",
        "        if source_names :\n",
        "            answer += f\"\\ nSources : {', '. join ( source_names )}\"\n",
        "        else :\n",
        "            answer += \"\\nNo sources found \"\n",
        "        await cl. Message ( content =answer , elements = text_elements ). send ()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! chainlit run app.py --host 0.0.0.0 --port 8000 &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib\n",
        "print (\" Password / Enpoint IP for localtunnel is:\",urllib . request . urlopen ('https://ipv4.icanhazip.com') . read () . decode ('utf8') . strip (\"\\n\") )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!lt --port 8000 -- subdomain aivn -simple -rag"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 513560,
          "sourceId": 1057373,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30732,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
